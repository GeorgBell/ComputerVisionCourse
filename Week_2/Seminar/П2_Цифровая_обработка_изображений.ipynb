{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bizarre-slovak",
   "metadata": {},
   "source": [
    "# Семинар 2\n",
    "# Основы цифровой обработки изображений\n",
    "# Курс \"Компьютерное зрение\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import plotly.graph_objects as go\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import scipy.ndimage as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-wallpaper",
   "metadata": {},
   "source": [
    "## 1. Представление изображений\n",
    "## 1.1. 2-D RGB изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb = io.imread(\"https://www.castlefineart.com/assets/img/resized/standard/the-starry-nightjpg.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-message",
   "metadata": {},
   "source": [
    "## 1.2. 2-D grayscale изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gray = io.imread(\"https://i.stack.imgur.com/B2DBy.jpg\", as_gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_gray, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-attraction",
   "metadata": {},
   "source": [
    "## 1.3. 3-D объекты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = io.imread(\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/attention-mri.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vol[:,:,0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = vol.T\n",
    "r, c = volume[0].shape\n",
    "r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frames\n",
    "nb_frames = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define figure\n",
    "fig = go.Figure(frames=[go.Frame(data=go.Surface(\n",
    "    z=(6.7 - k * 0.1) * np.ones((r, c)),\n",
    "    surfacecolor=np.flipud(volume[67 - k]),\n",
    "    cmin=0, cmax=200\n",
    "    ),\n",
    "    name=str(k) # you need to name the frame for the animation to behave properly\n",
    "    )\n",
    "    for k in range(nb_frames)])\n",
    "\n",
    "\n",
    "# Add data to be displayed before animation starts\n",
    "fig.add_trace(go.Surface(\n",
    "    z=6.7 * np.ones((r, c)),\n",
    "    surfacecolor=np.flipud(volume[67]),\n",
    "    colorscale='Gray',\n",
    "    cmin=0, cmax=200,\n",
    "    colorbar=dict(thickness=20, ticklen=4)\n",
    "    ))\n",
    "\n",
    "\n",
    "def frame_args(duration):\n",
    "    return {\n",
    "            \"frame\": {\"duration\": duration},\n",
    "            \"mode\": \"immediate\",\n",
    "            \"fromcurrent\": True,\n",
    "            \"transition\": {\"duration\": duration, \"easing\": \"linear\"},\n",
    "        }\n",
    "\n",
    "\n",
    "sliders = [\n",
    "            {\n",
    "                \"pad\": {\"b\": 10, \"t\": 60},\n",
    "                \"len\": 0.9,\n",
    "                \"x\": 0.1,\n",
    "                \"y\": 0,\n",
    "                \"steps\": [\n",
    "                    {\n",
    "                        \"args\": [[f.name], frame_args(0)],\n",
    "                        \"label\": str(k),\n",
    "                        \"method\": \"animate\",\n",
    "                    }\n",
    "                    for k, f in enumerate(fig.frames)\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "         title='Slices in volumetric data',\n",
    "         width=600,\n",
    "         height=600,\n",
    "         scene=dict(\n",
    "                    zaxis=dict(range=[-0.1, 6.8], autorange=False),\n",
    "                    aspectratio=dict(x=1, y=1, z=1),\n",
    "                    ),\n",
    "         updatemenus = [\n",
    "            {\n",
    "                \"buttons\": [\n",
    "                    {\n",
    "                        \"args\": [None, frame_args(50)],\n",
    "                        \"label\": \"&#9654;\", # play symbol\n",
    "                        \"method\": \"animate\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"args\": [[None], frame_args(0)],\n",
    "                        \"label\": \"&#9724;\", # pause symbol\n",
    "                        \"method\": \"animate\",\n",
    "                    },\n",
    "                ],\n",
    "                \"direction\": \"left\",\n",
    "                \"pad\": {\"r\": 10, \"t\": 70},\n",
    "                \"type\": \"buttons\",\n",
    "                \"x\": 0.1,\n",
    "                \"y\": 0,\n",
    "            }\n",
    "         ],\n",
    "         sliders=sliders\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-residence",
   "metadata": {},
   "source": [
    "## 2. Виды разрешения изображения\n",
    "## 2.1. Пространственное разрешение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_full = cv2.imread('clock.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "im_resizing = [cv2.resize(im_full, dsize=(int(im_full.shape[0]/i), \n",
    "                                          int(im_full.shape[1]/i))) for i in range(2,15,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20,15))\n",
    "axs[0].imshow(im_full, cmap=\"gray\")\n",
    "axs[0].title.set_text(f'Initial image: {im_full.shape}')\n",
    "\n",
    "for i, im in enumerate(im_resizing):\n",
    "    axs[i+1].imshow(im, cmap=\"gray\")\n",
    "    axs[i+1].title.set_text(f'Decreased image: {im.shape}')\n",
    "    \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-internship",
   "metadata": {},
   "source": [
    "## 2.2. Интерполяция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_interpol = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_LANCZOS4]\n",
    "resize_factor = 10\n",
    "im_resizing_interpol = [cv2.resize(im_full, dsize=(int(im_full.shape[0]/resize_factor), \n",
    "                                                   int(im_full.shape[1]/resize_factor)), \n",
    "                                   interpolation= interpol_type) for interpol_type in list_interpol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-company",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20,15))\n",
    "axs[0].imshow(im_full, cmap=\"gray\")\n",
    "axs[0].title.set_text(f'Initial image: {im_full.shape}')\n",
    "\n",
    "for i, im in enumerate(im_resizing_interpol):\n",
    "    axs[i+1].imshow(im, cmap=\"gray\")\n",
    "    axs[i+1].title.set_text(f'Decreased image: {im.shape}')\n",
    "    \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-retailer",
   "metadata": {},
   "source": [
    "## 3. Основные математические операции\n",
    "## 3.1. Произведение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_teeth = cv2.imread('teeth.png', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(im_teeth, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_mask = cv2.imread('mask_for_teeth.png', cv2.IMREAD_GRAYSCALE)\n",
    "_, im_mask_bin = cv2.threshold(im_mask,127,255,cv2.THRESH_BINARY)\n",
    "im_mask_bin = cv2.resize(im_mask_bin, dsize=(im_teeth.shape[1],im_teeth.shape[0])) / 255 \n",
    "plt.imshow(im_mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_mask_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-cargo",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_teeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_result = im_teeth * im_mask_bin\n",
    "plt.imshow(im_result, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-journalism",
   "metadata": {},
   "source": [
    "## 3.2. Суммирование\n",
    "Усреднение изображений позволяет избавиться от шума.  \n",
    "Создадим большое количество зашумленных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_clock = im_resizing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 127\n",
    "sigma = 100\n",
    "NoiseG = np.random.normal(mu, sigma, (im_clock.shape[0], im_clock.shape[1]))\n",
    "plt.imshow(NoiseG, cmap='gray')\n",
    "plt.title('Gaussian noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_clock_noised = (im_clock + NoiseG)/2\n",
    "plt.imshow(im_clock_noised, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 1000\n",
    "noisy_images = [(im_clock + np.random.normal(mu, sigma, (im_clock.shape[0], im_clock.shape[1])) / 2)\n",
    "               for i in range(num_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20,15))\n",
    "axs[0].imshow(im_clock, cmap=\"gray\")\n",
    "axs[0].title.set_text(f'Initial image')\n",
    "\n",
    "for i, im in enumerate(noisy_images[:3]):\n",
    "    axs[i+1].imshow(im, cmap=\"gray\")\n",
    "    axs[i+1].title.set_text(f'Noisy image')\n",
    "    \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_noised_images = [100, 500, 1000]\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20,15))\n",
    "axs[0].imshow(im_clock, cmap=\"gray\")\n",
    "axs[0].title.set_text(f'Initial image')\n",
    "\n",
    "for i, num in enumerate(num_noised_images):\n",
    "    axs[i+1].imshow(sum(noisy_images[:num])/num, cmap=\"gray\")\n",
    "    axs[i+1].title.set_text(f'Averaged {num} images')\n",
    "    \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-musical",
   "metadata": {},
   "source": [
    "## 3.3. Разность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_nocontr = cv2.imread('no_contrast.png', cv2.IMREAD_GRAYSCALE)\n",
    "im_contr = cv2.imread('contrast.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,15))\n",
    "axs[0].imshow(im_nocontr, cmap=\"gray\")\n",
    "axs[0].title.set_text(f'No contrast')\n",
    "axs[1].imshow(im_contr, cmap=\"gray\")\n",
    "axs[1].title.set_text(f'With contrast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_result = (im_nocontr/2 - im_contr/2)\n",
    "plt.imshow(im_result, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-essence",
   "metadata": {},
   "source": [
    "## 3.4. Аффинные преобразования\n",
    "OpenCV предлагает два подходна для геометрических трансформаций изображений: \n",
    "- афинных - cv.warpAffine (принимает в качестве аргумента матрицу преобразования 2х3)\n",
    "- преобразований в пространственной перспективе - cv.warpPerspective (принимает матрицу 3х3)  \n",
    "  \n",
    "Преобразование смещения:\n",
    "$$ M = \\begin{bmatrix}\n",
    "1 & 0 & t_{x}\\\\\n",
    "0 & 1 & t_{y}\\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('line_pic.png', cv2.IMREAD_GRAYSCALE)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = np.float32([[1, 0, 100], [0, 1, 50]])\n",
    "dst = cv2.warpAffine(img, M, (cols, rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,10))\n",
    "axs[0].imshow(img, cmap=\"gray\")\n",
    "axs[0].title.set_text(f'Initial')\n",
    "axs[1].imshow(dst, cmap=\"gray\")\n",
    "axs[1].title.set_text(f'Transformed image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-attribute",
   "metadata": {},
   "source": [
    "При аффинном преобразовании все параллельные линии в исходном изображении по-прежнему будут параллельны в выходном изображении. Чтобы найти матрицу преобразования, нам нужны три точки из входного изображения и их соответствующие местоположения в выходном изображении. Затем cv.getAffineTransform создаст матрицу 2x3, которая будет передана в cv.warpAffine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('line_pic.png')\n",
    "rows,cols,ch = img.shape\n",
    "\n",
    "pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "pts2 = np.float32([[10,100],[200,50],[100,250]])\n",
    "\n",
    "M = cv2.getAffineTransform(pts1,pts2)\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-grill",
   "metadata": {},
   "source": [
    "Для преобразования в перспективе требуется матрица преобразования 3x3. Прямые линии останутся прямыми даже после трансформации. Чтобы найти эту матрицу преобразования, нам нужны 4 точки на входном изображении и соответствующие точки на выходном изображении. Среди этих 4 точек 3 из них не должны лежать на одной прямой. Тогда матрицу преобразования можно найти с помощью функции cv.getPerspectiveTransform. Затем примените cv.warpPerspective с этой матрицей преобразования 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('sudoku.png')\n",
    "\n",
    "rows,cols,ch = img.shape\n",
    "\n",
    "pts1 = np.float32([[115,130],[746,106],[60,775],[788,784]])\n",
    "pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "dst = cv2.warpPerspective(img,M,(300,300))\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-camping",
   "metadata": {},
   "source": [
    "## 3.5. Статистические метрики\n",
    "Оценка дисперсии изображений с целью оценки сфокусированности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_images = []\n",
    "variance_vals = []\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20,15))\n",
    "\n",
    "for i in range(1, 6):\n",
    "    img = cv2.imread(f'{i}.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    var_val = np.var(img)\n",
    "    \n",
    "    axs[i-1].imshow(img, cmap=\"gray\")\n",
    "    axs[i-1].title.set_text(f'Variance value: {var_val :.2f}')\n",
    "    \n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-speed",
   "metadata": {},
   "source": [
    "## 4. Пространственная фильтрация\n",
    "## 4.1. Свертка\n",
    "<img src=\"conv1.gif\" width=\"400\" height=\"400\">\n",
    "<img src=\"conv2.gif\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-roulette",
   "metadata": {},
   "source": [
    "Как и в случае одномерных сигналов, изображения также могут быть отфильтрованы с помощью различных фильтров нижних частот (LPF), фильтров верхних частот (HPF) и т. д. LPF помогает удалять шум, размывать изображения, фильтры HPF помогают находить края в изображений.  \n",
    "\n",
    "OpenCV предоставляет функцию cv.filter2D () для свертки ядра с изображением. В качестве примера мы попробуем использовать усредняющий фильтр на изображении. Ядро усредняющего фильтра 5x5 будет выглядеть следующим образом:\n",
    "$$ K = 1/25\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5,5), np.float32)/25\n",
    "dst = cv2.filter2D(image_gray,-1,kernel)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(121),plt.imshow(image_gray, cmap=\"gray\"),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst, cmap=\"gray\"),plt.title('Averaging')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-ideal",
   "metadata": {},
   "source": [
    "Реализуем функцию для расчета 2-D свертки с ядром 3х3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(image, kernel):\n",
    "    # convolution output\n",
    "    output = np.zeros_like(image)\n",
    "\n",
    "    # Add zero padding to the input image\n",
    "    image_padded = np.zeros((image.shape[0] + 2, image.shape[1] + 2))\n",
    "    image_padded[1:-1, 1:-1] = image\n",
    "\n",
    "    # Loop over every pixel of the image\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            # element-wise multiplication of the kernel and the image\n",
    "            output[y, x]=(kernel * image_padded[y: y+3, x: x+3]).sum()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3,3), np.float32)/9\n",
    "dst_manual = convolve2d(image_gray,kernel)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(131),plt.imshow(image_gray, cmap=\"gray\"),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(dst, cmap=\"gray\"),plt.title('Averaging builtin')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(dst, cmap=\"gray\"),plt.title('Averaging manual')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-albany",
   "metadata": {},
   "source": [
    "## 4.2. Сглаживание изображений\n",
    "Применим нелинейный метод фильтрации, выполняющийся, как и в предыдущем случае, в скользящем окне. \n",
    "Применим встроенную функцию cv.medianBlur(), которая определяет медианное значение всех пикселей под областью ядра, а центральный элемент заменяется этим медианным значением. Это очень эффективно против шума по типу \"солью и перцем\" на изображении. Это эффективно снижает шум. Размер ядра должен быть положительным нечетным целым числом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_noise = io.imread(\"img_noise.png\")\n",
    "image_median = cv2.medianBlur(image_noise, 9)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(121),plt.imshow(image_noise),plt.title('Noised')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(image_median),plt.title('After median filtering')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-victoria",
   "metadata": {},
   "source": [
    "Реализуем вручную медианную фильтрацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_noise = io.imread(\"img_noise.png\", as_gray=True)\n",
    "image_noise = cv2.resize(image_noise, dsize=(300,300))\n",
    "\n",
    "w = 5\n",
    "\n",
    "image_median_man = np.empty([image_noise.shape[0], image_noise.shape[1]])\n",
    "for i in range(0, image_noise.shape[0]):\n",
    "    for j in range(0, image_noise.shape[1]):\n",
    "        image_median_man[i,j] = np.median(image_noise[(i-int(w/2)):(i+int(w/2)), (j-int(w/2)):(j+int(w/2))])\n",
    "        \n",
    "plt.imshow(image_median_man, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Manual implementation of median filtering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-engine",
   "metadata": {},
   "source": [
    "## 4.3. Вычисление градиентов\n",
    "Выделим границы с помощью матрицы Лапласа. Такой подход позволяет выделить за один проход границы различной пространственной ориентации:\n",
    "$$ kernel = \\begin{bmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "1 & -4 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sudoku = cv2.imread(\"cross.png\", 0)\n",
    "\n",
    "laplacian = cv2.Laplacian(image_sudoku, cv2.CV_64F)\n",
    "sobelx = cv2.Sobel(image_sudoku, cv2.CV_64F, 1, 0, ksize=5)\n",
    "sobely = cv2.Sobel(image_sudoku, cv2.CV_64F, 0, 1, ksize=5)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,2,1), plt.imshow(image_sudoku, cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,2),plt.imshow(laplacian, cmap = 'gray')\n",
    "plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,3),plt.imshow(sobelx, cmap = 'gray')\n",
    "plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,4),plt.imshow(sobely, cmap = 'gray')\n",
    "plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-artwork",
   "metadata": {},
   "source": [
    "## 4.4. Поиск шаблона на изображении\n",
    "Сопоставление шаблонов - это метод поиска и определения местоположения изображения шаблона на исходном изображении. OpenCV предлагает функцию cv.matchTemplate() для этой цели. Идея заключается в прохождении шаблона по входному изображению (как при двумерной свертке) и сравнении шаблона с фрагментом входного изображения. В OpenCV реализовано несколько методов сравнения. Функция возвращает изображение в градациях серого, где каждый пиксель обозначает, насколько данная область соответствует шаблону."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('messi.jpg',0)\n",
    "img2 = img.copy()\n",
    "template = cv2.imread('template.jpg',0)\n",
    "\n",
    "plt.imshow(template,cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = template.shape[::-1]\n",
    "\n",
    "meth = 'cv2.TM_CCOEFF'\n",
    "\n",
    "img = img2.copy()\n",
    "method = eval(meth)\n",
    "\n",
    "# Apply template Matching\n",
    "res = cv2.matchTemplate(img,template,method)\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "top_left = max_loc\n",
    "bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "cv2.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121),plt.imshow(res,cmap = 'gray')\n",
    "plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-queen",
   "metadata": {},
   "source": [
    "## 5. Яркостные преобразования\n",
    "## 5.1. Основы\n",
    "<img src=\"Graph.png\" width=\"400\" height=\"400\">\n",
    "<img src=\"Graph1.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_breast = cv2.imread('breast.png',0)\n",
    "img_breast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "img_breast_negative = 255 - img_breast\n",
    "img_breast_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(121),plt.imshow(img_breast,cmap = 'gray')\n",
    "plt.title('Initial image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(img_breast_negative,cmap = 'gray')\n",
    "plt.title('Negative image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-attribute",
   "metadata": {},
   "source": [
    "## 5.2. Гистограмма изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scene = cv2.imread('scene.png',0)\n",
    "hist_full = cv2.calcHist([img_scene],[0],None,[256],[0,256])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121), plt.imshow(img_scene, 'gray')\n",
    "plt.subplot(122), plt.plot(hist_full),plt.xlim([0,256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-tulsa",
   "metadata": {},
   "source": [
    "## 5.3. Эквализация гистограммы\n",
    "<img src=\"hist_eq.png\" width=\"400\" height=\"400\">\n",
    "  \n",
    "Выполним глобальную эквализацию гистограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_eq_glob = cv2.equalizeHist(img_scene)\n",
    "hist_eq_scene = cv2.calcHist([img_eq_glob],[0],None,[256],[0,256])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121), plt.imshow(img_eq_glob, 'gray')\n",
    "plt.subplot(122), plt.plot(hist_eq_scene),plt.xlim([0,256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-canberra",
   "metadata": {},
   "source": [
    "Выполним адаптивную локальную эквализацию гистограммы: CLAHE (Contrast Limited Adaptive Histogram Equalization).   \n",
    "В результате глобальной эквализации мы потеряли часть информации из-за чрезмерной яркости (лицо). Для решения этой проблемы используется адаптивное выравнивание гистограммы. В этом случае изображение разделяется на небольшие блоки, называемые «плитками» (размер плитки по умолчанию в OpenCV равен 8x8). Затем для каждого из этих блоков выравнивается гистограмма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "img_clahe = clahe.apply(img_scene)\n",
    "hist_eq_clahe = cv2.calcHist([img_clahe],[0],None,[256],[0,256])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121), plt.imshow(img_clahe, 'gray')\n",
    "plt.subplot(122), plt.plot(hist_eq_clahe),plt.xlim([0,256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-potato",
   "metadata": {},
   "source": [
    "## 5.4. Извлечение объектов по гистограмме\n",
    "Извлечение объектов по гистограмме (Histogram backprojection) было предложено Майклом Дж. Суэйном, Даной Х. Баллард в их статье «Indexing via color histograms».\n",
    "\n",
    "Данный подход используется для сегментации изображения или поиска интересующих объектов на изображении. В результате применения такого метода создается изображение того же размера (но с одним каналом), что и у входного изображения, где каждый пиксель соответствует вероятности того, что он принадлежит искомому объекту.   \n",
    "\n",
    "OpenCV предоставляет встроенную функцию cv.calcBackProject(). Ее параметры почти такие же, как у функции cv.calcHist(). Один из его параметров - гистограмма искомого объекта. Кроме того, гистограмма объекта должна быть нормализована перед применением функции backproject. Функция возвращает вероятностное изображение, которое далее сворачивается с исходным изображением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = cv2.imread('temp_hist.jpg') \n",
    "hsv = cv2.cvtColor(roi,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "target = cv2.imread('messi.jpg')\n",
    "hsvt = cv2.cvtColor(target,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# calculating object histogram\n",
    "roihist = cv2.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )\n",
    "\n",
    "# normalize histogram and apply backprojection\n",
    "cv2.normalize(roihist,roihist,0,255,cv2.NORM_MINMAX)\n",
    "dst = cv2.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)\n",
    "\n",
    "# Now convolute with circular disc\n",
    "disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n",
    "cv2.filter2D(dst,-1,disc,dst)\n",
    "\n",
    "# threshold and binary AND\n",
    "ret,thresh = cv2.threshold(dst,50,255,0)\n",
    "thresh = cv2.merge((thresh,thresh,thresh))\n",
    "res = cv2.bitwise_and(target,thresh)\n",
    "\n",
    "# Merge images\n",
    "res = cv2.cvtColor(np.vstack((target,thresh,res)), cv2.COLOR_BGR2RGB) \n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-purpose",
   "metadata": {},
   "source": [
    "## Задание\n",
    "1. С помощью OpenCV и веб-камеры получить изображение документа на листе A4\n",
    "2. Перевести изображение в серошкальный формат\n",
    "3. Определить координаты угловых точек документа с помощью интерактивный утилиты matplotlib (см. семинар 1)\n",
    "4. Выполнить преобразование перспективы так, чтобы плоскость документа была выровнена (эффект сканирования)\n",
    "5. Провести адаптивную эквализацию гистограммы\n",
    "6. Выделить на изображении все границы\n",
    "7. Получить маску по границам\n",
    "8. Применить маску к изображению, полученному на этапе 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
